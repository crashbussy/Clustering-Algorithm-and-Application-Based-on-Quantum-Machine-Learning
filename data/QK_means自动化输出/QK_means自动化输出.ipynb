{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math lib\n",
    "from math import pi\n",
    "\n",
    "# import Qiskit\n",
    "from qiskit import Aer, execute#aer是模拟器\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "\n",
    "# import basic plot tools\n",
    "from qiskit.tools.visualization import plot_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = Aer.get_backend('aer_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta(d):\n",
    "    i = 0\n",
    "    theta = 0\n",
    "    count = 0\n",
    "    for i in d:\n",
    "        count = i+count\n",
    "   # print(d)\n",
    "    theta = 2*math.acos(count/384)\n",
    "   # print(theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Distance(x,y):\n",
    "    theta_1 = get_theta(x)\n",
    "    theta_2 = get_theta(y)\n",
    "    \n",
    "    qr = QuantumRegister(3, name=\"qr\")\n",
    "    cr = ClassicalRegister(1, name=\"cr\")\n",
    "    qc = QuantumCircuit(qr, cr, name=\"k_means\")\n",
    "    \n",
    "    qc.h(qr[0])\n",
    "    qc.h(qr[1])\n",
    "    qc.h(qr[2])\n",
    "    qc.u(theta_1, pi, pi, qr[1])\n",
    "    qc.u(theta_2, pi, pi, qr[2])\n",
    "    qc.cswap(qr[0], qr[1], qr[2])\n",
    "    qc.h(qr[0])\n",
    "\n",
    "    qc.measure(qr[0], cr[0])\n",
    "    qc.reset(qr)\n",
    "\n",
    "    job = execute(qc,backend=backend, shots=1024)\n",
    "    result = job.result()\n",
    "    data = result.data()['counts']\n",
    "    \n",
    "    if len(data)==1:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return data['0x1']/1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_name,model_name,data_num):\n",
    "    import datasets\n",
    "    dataset = datasets.load_from_disk(dataset_name)\n",
    "    from sentence_transformers import SentenceTransformer \n",
    "    model = SentenceTransformer(model_name,cache_folder=r\"D:\\HF-model\\all-MiniLM-L6-v2\")\n",
    "    from random import sample \n",
    "    import random\n",
    "    n = random.randint(1,999999)  \n",
    "    random.seed(n)\n",
    "    sentences = sample(dataset['test'][0]['sentences'],data_num) \n",
    "    random.seed(n) \n",
    "    labels =  sample(dataset['test'][0]['labels'],data_num)\n",
    "    embeddings = model.encode(sentences)#\n",
    "    points=embeddings\n",
    "    #初始化聚类中心\n",
    "    np.random.seed(96)\n",
    "    centroids = np.random.random([32,384])\n",
    "    return points,centroids,sentences,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1-p2)*(p1-p2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbour(points,centroids):\n",
    "    \n",
    "    n = len(points)\n",
    "    k = centroids.shape[0]\n",
    "    centers = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        min_dis = 10000\n",
    "        ind = 0\n",
    "        for j in range(k):\n",
    "            temp_dis = get_Distance(points[i,:],centroids[j,:])\n",
    "            \n",
    "            if temp_dis < min_dis:\n",
    "                min_dis = temp_dis\n",
    "                ind = j\n",
    "        centers[i] = ind\n",
    "    \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroids(points,centers):\n",
    "    n = len(points)\n",
    "    k = int(np.max(centers))+1   \n",
    "    centroids = np.zeros([k,384])\n",
    "    for i in range(k):\n",
    "        centroids[i,:] = np.average(points[centers==i])\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(points):\n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    scaler = MinMaxScaler() \n",
    "    normalized_data = scaler.fit_transform(points)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n可选数据集：\\n'D:\\\\HF dataset\\\\mteb/arxiv-clustering-p2p'\\n'D:\\\\HF dataset\\\\mteb/arxiv-clustering-s2s'\\n'D:\\\\HF dataset\\\\mteb/biorxiv-clustering-p2p'\\n'D:\\\\HF dataset\\\\mteb/biorxiv-clustering-s2s'\\n'D:\\\\HF dataset\\\\mteb/medrxiv-clustering-s2s'\\n'D:\\\\HF dataset\\\\mteb/reddit-clustering'\\n'D:\\\\HF dataset\\\\mteb/reddit-clustering-p2p'\\n'D:\\\\HF dataset\\\\mteb/stackExchange-clustering'\\n'D:\\\\HF dataset\\\\mteb/stackExchange-clustering-p2p'\\n'D:\\\\HF dataset\\\\mteb/twentynewsgroups-clustering'\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "可选数据集：\n",
    "'D:\\HF dataset\\mteb/arxiv-clustering-p2p'\n",
    "'D:\\HF dataset\\mteb/arxiv-clustering-s2s'\n",
    "'D:\\HF dataset\\mteb/biorxiv-clustering-p2p'\n",
    "'D:\\HF dataset\\mteb/biorxiv-clustering-s2s'\n",
    "'D:\\HF dataset\\mteb/medrxiv-clustering-s2s'\n",
    "'D:\\HF dataset\\mteb/reddit-clustering'\n",
    "'D:\\HF dataset\\mteb/reddit-clustering-p2p'\n",
    "'D:\\HF dataset\\mteb/stackExchange-clustering'\n",
    "'D:\\HF dataset\\mteb/stackExchange-clustering-p2p'\n",
    "'D:\\HF dataset\\mteb/twentynewsgroups-clustering'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\Tool\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#range中的t1：重复实验次数（每一遍都是同样大小数据集，但是选择的数据不同）\n",
    "#t2 为QKmeans算法中迭代次数\n",
    "t1 = 3\n",
    "t2 = 3\n",
    "for qq in range(1):\n",
    "    output = np.array([['v_measure','标签种类(k)']])\n",
    "        #选择数据集的模块\n",
    "    if qq==0:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/biorxiv-clustering-p2p'\n",
    "    if qq==1:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/biorxiv-clustering-s2s'\n",
    "    if qq==2:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/medrxiv-clustering-s2s'\n",
    "    if qq==3:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/reddit-clustering'\n",
    "    if qq==4:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/reddit-clustering-p2p'\n",
    "    if qq==5:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/stackExchange-clustering'\n",
    "    if qq==6:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/stackExchange-clustering-p2p'\n",
    "    if qq==7:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/twentynewsgroups-clustering'\n",
    "    if qq==8:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/arxiv-clustering-p2p'\n",
    "    if qq==9:\n",
    "        dataset_name = 'D:\\HF dataset\\mteb/arxiv-clustering-s2s'    \n",
    "    #QKmeans\n",
    "    for n1 in range(t1):\n",
    "\n",
    "        #设置超参数\n",
    "        model_name = 'all-MiniLM-L6-v2'\n",
    "        data_num = 100\n",
    "    \n",
    "        points,centroids1,sentences,labels = get_data(dataset_name,model_name,data_num)       #dataset\n",
    "\n",
    "        points = preprocess(points)                # Normalize dataset\n",
    "        # run k-means algorithm\n",
    "        for i1 in range(t2):\n",
    "            centers = find_nearest_neighbour(points,centroids1)       # find nearest centers\n",
    "            centroids = find_centroids(points,centers)               # find centroids\n",
    "        #计算这组数据集中有多少不同种标签\n",
    "        k=[0]*len(labels)\n",
    "        count = 0\n",
    "        flag1 = 0\n",
    "        for i in labels: \n",
    "            flag1 = 0 \n",
    "            for j in k: \n",
    "                if j == i: \n",
    "                    flag1 = 0\n",
    "                    break \n",
    "                else:  \n",
    "                    flag1 = 1\n",
    "            if flag1 == 1:\n",
    "                for n in range(len(labels)):\n",
    "                    if k[n] == 0:\n",
    "                        k[n] = i\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "        for m in k:\n",
    "            if m == 0:\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1   \n",
    "    \n",
    "        #评价部分\n",
    "        from mteb.evaluation.evaluators import ClusteringEvaluator\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2',cache_folder=r\"D:\\HF-model\\all-MiniLM-L6-v2\")\n",
    "        labels = centers\n",
    "        clusterer = ClusteringEvaluator(sentences=sentences, labels=labels)\n",
    "        #result是dic类型\n",
    "        result = clusterer(model)\n",
    "        output = np.insert(output,n1+1,[str(result['v_measure']),str(count)],axis=0)\n",
    "    output = np.insert(output,0,[dataset_name[19:],str(data_num)],axis=0)\n",
    "\n",
    "\n",
    "    filename = '%s组重复 %s %s.csv'%(t1,dataset_name[19:],model_name)\n",
    "    np.savetxt(filename,output,fmt='%s',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-pytorch]",
   "language": "python",
   "name": "conda-env-Anaconda3-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
